{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@author Adrian Spork\n",
    "#@author Tatjana Melina Walter\n",
    "\n",
    "# todo: Bei einem Datensatz muss die Benennung am ende passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import getpass\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from time import sleep\n",
    "import stat\n",
    "import io\n",
    "from rasterio.enums import Resampling\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as request\n",
    "from contextlib import closing\n",
    "from ftplib import FTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Sentinel2########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadingData(aoi, collectionDate, plName, prLevel, clouds, username, password, directory):\n",
    "    '''\n",
    "    Downloads the Sentinel2 - Data with the given parameters\n",
    "\n",
    "    Parameters:\n",
    "        aoi (str): The type and the coordinates of the area of interest\n",
    "        collectionDate datetime 64[ns]): The date of the data\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The name of the process\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        username (str): The username of the Copernicus SciHub\n",
    "        password (str): The password of the Copernicus SciHub\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "    \n",
    "    api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')\n",
    "    \n",
    "    '''Choosing the data with bounding box (footprint), date, platformname, processinglevel and cloudcoverpercentage'''\n",
    "    products = api.query(aoi, date = collectionDate, platformname = plName, processinglevel = prLevel, cloudcoverpercentage = clouds)\n",
    "\n",
    "    '''Filters the products and sorts by cloudcoverpercentage'''\n",
    "    products_gdf = api.to_geodataframe(products)\n",
    "    products_gdf_sorted = products_gdf.sort_values(['cloudcoverpercentage'], ascending = [True])\n",
    "\n",
    "    '''Downloads the choosen files from Scihub'''\n",
    "    products_gdf_sorted.to_csv(os.path.join(directory, 'w'))\n",
    "    api.download_all(products, directory, max_attempts = 10, checksum = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipping(filename, directory):\n",
    "    '''\n",
    "    Unzips the file with the given filename\n",
    "\n",
    "    Parameter:\n",
    "        filename(str): Name of the .zip file\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "    with ZipFile(os.path.join(directory, filename), 'r') as zipObj:\n",
    "        zipObj.extractall(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(directory):\n",
    "    '''\n",
    "    Unzips and deletes the .zip in the given directory\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".zip\"):\n",
    "            if(filename[39:41]!=\"32\"):\n",
    "                print(\"CRS not supported! Only EPSG:32632 supported\") #do not throw an exception here\n",
    "                delete(os.path.join(directory,filename))\n",
    "            else:\n",
    "                unzipping(filename, directory)\n",
    "                delete(os.path.join(directory, filename))\n",
    "                continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractBands(filename, resolution, directory):\n",
    "    '''\n",
    "    Extracts bandpaths from the given .SAFE file\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Sentinel .SAFE file\n",
    "        resolution (int): The resolution the datacube should have\n",
    "        directory (str): Pathlike string to the directory\n",
    "\n",
    "    Returns:\n",
    "        bandPaths (str[]): An array of the paths for the red and nir band\n",
    "    '''\n",
    "\n",
    "    lTwoA = os.listdir(os.path.join(directory, filename, \"GRANULE\"))\n",
    "\n",
    "    if resolution == 10:\n",
    "        bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[4]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "    elif resolution == 20:\n",
    "        bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "    elif resolution == 60:\n",
    "        bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[4]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[11]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "    elif resolution == 100:\n",
    "        bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "    else:\n",
    "        print(\"No such resolution\")\n",
    "        return -1\n",
    "\n",
    "    return bandPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBand (bandpath, date, tile, resolution, clouds, plName, prLevel, directory):\n",
    "    '''\n",
    "    Opens and reads the red and nir band, saves them as NetCDF file\n",
    "\n",
    "    Parameters:\n",
    "        bandPaths (str[]): Array with the paths to the red and nir band\n",
    "        date (datetime 64[ns]): The collection date (\"2020-12-31\")\n",
    "        tile (str): Bounding box of coordinates defined by Sentinel\n",
    "        resolution (int): The resolution of the dataset\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The level of the process\n",
    "        directory (str): Pathlike string to the directory\n",
    "\n",
    "    Returns:\n",
    "        dataset (xArray dataset): The result dataset as xArray dataset\n",
    "    '''\n",
    "\n",
    "    b4 = rio.open(bandpath[0])\n",
    "    b8 = rio.open(bandpath[1])\n",
    "    red = b4.read()\n",
    "    nir = b8.read()\n",
    "\n",
    "    if resolution == 10:\n",
    "        res = 1830 * 3 * 2\n",
    "    elif resolution == 20:\n",
    "        res = 1830 * 3\n",
    "    elif resolution == 60:\n",
    "        res = 1830\n",
    "    elif resolution == 100:\n",
    "        res = 1098\n",
    "    else:\n",
    "        print(\"No such resolution\")\n",
    "        return -1\n",
    "\n",
    "    j = res - 1\n",
    "    i = 0\n",
    "    lat = [0] * res\n",
    "    lon = [0] * res\n",
    "    while j >= 0:\n",
    "        lon[i] = b4.bounds.left + i * resolution\n",
    "        lat[i] = b4.bounds.bottom + j * resolution\n",
    "        i = i + 1\n",
    "        j = j - 1\n",
    "\n",
    "    time = pd.date_range(date, periods = 1)\n",
    "\n",
    "    if resolution == 100:\n",
    "        upscale_factor = (1/5)\n",
    "        nir = b8.read(\n",
    "                out_shape = (\n",
    "                    b8.count,\n",
    "                    int(b8.height * upscale_factor),\n",
    "                    int(b8.width * upscale_factor)\n",
    "                ),\n",
    "                resampling = Resampling.bilinear\n",
    "        )\n",
    "        transform = b8.transform * b8.transform.scale(\n",
    "            (b8.width / nir.shape[-1]),\n",
    "            (b8.height / nir.shape[-2])\n",
    "        )\n",
    "        red = b4.read(\n",
    "            out_shape = (\n",
    "                b4.count,\n",
    "                int(b4.height * upscale_factor),\n",
    "                int(b4.width * upscale_factor)\n",
    "            ),\n",
    "            resampling = Resampling.bilinear\n",
    "        )\n",
    "\n",
    "        transform = b4.transform * b4.transform.scale(\n",
    "            (b4.width / red.shape[-1]),\n",
    "            (b4.height / red.shape[-2])\n",
    "        )\n",
    "\n",
    "    dataset = xr.Dataset(\n",
    "        {\n",
    "            \"red\": ([\"time\",\"lat\", \"lon\"], red),\n",
    "            \"nir\": ([\"time\",\"lat\", \"lon\"], nir)\n",
    "        },\n",
    "        coords = dict(\n",
    "            time = time,\n",
    "            lat = ([\"lat\"], lat),\n",
    "            lon = ([\"lon\"], lon),\n",
    "        ),\n",
    "        attrs = dict(\n",
    "            platform = plName,\n",
    "            processingLevel = prLevel,\n",
    "            cloudcover = clouds,\n",
    "            source = \"https://scihub.copernicus.eu/dhus\",\n",
    "            resolution = str(resolution) + \" x \" + str(resolution) + \" m\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dataset.to_netcdf(directory + \"datacube_\" + str(date) + \"_\" + str(tile) + \"_R\" + str(resolution) + \".nc\", 'w', format = 'NETCDF4')\n",
    "    b4.close()\n",
    "    b8.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(filename):\n",
    "    '''\n",
    "    Extracts the Date out of the Sentinelfilename\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Name of the file\n",
    "\n",
    "    Returns:\n",
    "        (str): Date of the File (\"2020-12-31\")\n",
    "    '''\n",
    "\n",
    "    return filename[11:15] + \"-\" + filename[15:17] + \"-\" + filename[17:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTile(filename):\n",
    "    '''\n",
    "    Extracts the UTM-tile of the Sentinelfilename\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Name of the file\n",
    "\n",
    "    Returns:\n",
    "        (str): UTM-tile of the File (\"31UMC\")\n",
    "    '''\n",
    "    return filename[38:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_rm_error(func, path, exc_info):\n",
    "    '''\n",
    "    Unlinks a read-only file\n",
    "    '''\n",
    "\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    os.unlink(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCube(directory, resolution, clouds, plName, prLevel):\n",
    "    '''\n",
    "    Builds a datacube in the given directory with coords, time as dimensions and the bands as datavariables\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string to the directory\n",
    "        resolution (int): The resolution of the dataset\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The level of the process\n",
    "    '''\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".SAFE\"):\n",
    "            bandPath = extractBands(os.path.join(directory, filename), resolution, directory)\n",
    "            band = loadBand(bandPath, getDate(filename), getTile(filename), resolution, clouds, plName, prLevel, directory)\n",
    "            shutil.rmtree(os.path.join(directory, filename), onerror = on_rm_error)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Sentinel(directory):\n",
    "    '''\n",
    "    Merges datacubes by coordinates and time\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string where Data is stored\n",
    "    '''\n",
    "\n",
    "    start = datetime.now()\n",
    "    count1 = 0\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(\"Directory empty\")\n",
    "        return\n",
    "    elif len(files) == 1:\n",
    "        print(\"Only one file in directory\")\n",
    "        return\n",
    "    else:\n",
    "        print('Start merging')\n",
    "        for file1 in files:\n",
    "            if count1 == len(files):\n",
    "                return\n",
    "            for file2 in files:\n",
    "                count2 = 0\n",
    "                if file1.endswith(\".nc\") and file2.endswith(\".nc\"):\n",
    "                    file1Date = file1[9:19]\n",
    "                    file1Tile = file1[20:26]\n",
    "                    file1Res = file1[27:31]\n",
    "                    file2Date = file2[9:19]\n",
    "                    file2Tile = file2[20:26]\n",
    "                    file2Res = file2[27:31]\n",
    "                    if file1[21:23] == \"31\":\n",
    "                        delete(os.path.join(directory,file1))\n",
    "                    elif file2[21:23] == \"31\":\n",
    "                        delete(os.path.join(directory,file2))\n",
    "                    elif file1Date == file2Date and file1Tile == file2Tile and file1Res == file2Res:\n",
    "                        continue\n",
    "                    elif file1Date == file2Date and file1Tile == \"T32ULC\" and file2Tile == \"T32UMC\" and file1Res == file2Res:\n",
    "                        fileLeft = xr.open_dataset(os.path.join(directory, file1))\n",
    "                        fileRight = xr.open_dataset(os.path.join(directory, file2))\n",
    "                        merge_coords(fileLeft, fileRight, file1[0:20] + \"Merged\" + file1[26:31], directory)\n",
    "                        fileLeft.close()\n",
    "                        fileRight.close()\n",
    "                        delete(os.path.join(directory, file1))\n",
    "                        delete(os.path.join(directory, file2))\n",
    "                        continue\n",
    "                else:\n",
    "                    raise TypeError(\"Wrong file in directory\") \n",
    "                    \n",
    "\n",
    "        files = os.listdir(directory)\n",
    "        while len(os.listdir(directory)) > 1:\n",
    "            files = os.listdir(directory)\n",
    "            if files[0].endswith(\".nc\") and files[1].endswith(\".nc\"):\n",
    "                file1 = xr.open_dataset(os.path.join(directory, files[0]))\n",
    "                file2 = xr.open_dataset(os.path.join(directory, files[1]))\n",
    "                merge_time(file1, file2, files[0][0:31], directory)\n",
    "                file1.close()\n",
    "                file2.close()\n",
    "                delete(os.path.join(directory, files[1]))\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Error: Wrong file in directory\")\n",
    "                raise TypeError(\"Wrong file in directory\") \n",
    "\n",
    "    end = datetime.now()\n",
    "    diff = end - start\n",
    "    print('All cubes merged for ' + str(diff.seconds) + 's')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeframe(ds, start, end):\n",
    "    '''\n",
    "    Slices Datacube down to given timeframe\n",
    "\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Sourcedataset\n",
    "        start (str): Start of the timeframe eg '2018-07-13'\n",
    "        end (str): End of the timeframe eg '2018-08-23'\n",
    "\n",
    "    Returns:\n",
    "        ds_selected (xArray Dataset): Dataset sliced to timeframe\n",
    "    '''\n",
    "\n",
    "    if start > end:\n",
    "        print(\"start and end of the timeframe are not compatible!\")\n",
    "    else:\n",
    "        ds_selected = ds.sel(time = slice(start, end))\n",
    "        return ds_selected  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_datacube(ds, name, directory):\n",
    "    '''\n",
    "    Saves the Datacube as NetCDF (.nc)\n",
    "\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Sourcedataset\n",
    "        name (str): Name eg '2017', '2015_2019'\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    print(\"Start saving\")\n",
    "    start = datetime.now()\n",
    "    if type(name) != str:\n",
    "        name = str(name)\n",
    "    ds.to_netcdf(directory + name + \".nc\")\n",
    "    diff = datetime.now() - start\n",
    "    print(\"Done saving after \"+ str(diff.seconds) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_coords(ds_left, ds_right, name, directory):\n",
    "    '''\n",
    "    Merges two datasets by coordinates\n",
    "\n",
    "    Parameters:\n",
    "        ds_left (xArray dataset): Dataset to be merged\n",
    "        ds_right (xArray dataset): Dataset to be merged\n",
    "        name (str): Name of the new dataset\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    ds_selected = slice_lon(ds_left, ds_left.lon[0], ds_right.lon[0])\n",
    "    ds_merge = [ds_selected, ds_right]\n",
    "    merged = xr.combine_by_coords(ds_merge)\n",
    "    safe_datacube(merged, name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time(ds1, ds2, name, directory):\n",
    "    '''\n",
    "    Merges two datasets by time\n",
    "\n",
    "    Parameters:\n",
    "        ds1 (xArray dataset): Dataset to be merged\n",
    "        ds2 (xArray dataset): Dataset to be merged\n",
    "        name (str): Name of the new dataset\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    res = xr.combine_by_coords([ds1, ds2])\n",
    "    ds1.close()\n",
    "    ds2.close()\n",
    "    safe_datacube(res, name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_lat(ds, lat_left, lat_right):\n",
    "    '''\n",
    "    Slices a given dataset to given latitude bounds\n",
    "\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Dataset to be sliced\n",
    "        lat_left (float): Left latitude bound\n",
    "        lat_right (float): Right latitude bound\n",
    "\n",
    "    Returns:\n",
    "        ds (xArray Dataset): Sliced dataset\n",
    "    '''\n",
    "\n",
    "    ds_selected = ds.sel(lat = slice(lat_left, lat_right))\n",
    "    return ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_lon(ds, lon_left, lon_right):\n",
    "    '''\n",
    "    Slices a given dataset to given longitude bounds\n",
    "\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Dataset to be sliced\n",
    "        lon_left (float): Left longitude bound\n",
    "        lon_right (float): Right longitude bound\n",
    "\n",
    "    Returns:\n",
    "        ds (xArray Dataset): Sliced dataset\n",
    "    '''\n",
    "\n",
    "    ds_selected = ds.sel(lon = slice(lon_left, lon_right))\n",
    "    return ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_coords(ds, lon_left, lon_right, lat_left, lat_right):\n",
    "    '''\n",
    "    Slices a dataset to a given slice\n",
    "\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Dataset to be sliced\n",
    "        lon_left (float): Left bound for longitude\n",
    "        lon_right (float): Right bound for longitude\n",
    "        lat_left (float): Left bound for latitude\n",
    "        lat_right (float): Right bound for latitude\n",
    "\n",
    "    Returns:\n",
    "        ds (xArray Dataset): Sliced dataset\n",
    "    '''\n",
    "\n",
    "    ds_selected = slice_lon(ds, lon_left, lon_right)\n",
    "    return slice_lat(ds_selected, lat_left, lat_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(path):\n",
    "    '''\n",
    "    Deletes the file/directory with the given path\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the file/directory\n",
    "    '''\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(\"File deleted: \" + path)\n",
    "    else:\n",
    "        print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainSentinel(resolution, directory, collectionDate, aoi, clouds, username, password):\n",
    "    '''\n",
    "    Downloads, unzips, collects and merges Sentinel2 Satelliteimages to a single netCDF4 datacube\n",
    "\n",
    "    Parameters:\n",
    "        resolution (int): Resolution of the satelite image\n",
    "        directory (str): Pathlike string to the workdirectory\n",
    "        collectionDate (tuple of datetime 64[ns]): Start and end of the timeframe\n",
    "        aoi (POLYGON): Area of interest\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        username (str): Uername for the Copernicus Open Acess Hub\n",
    "        password (str): Password for the Copernicus Open Acess Hub\n",
    "    '''\n",
    "\n",
    "    plName = 'Sentinel-2'\n",
    "    prLevel = 'Level-2A'\n",
    "    downloadingData (aoi, collectionDate, plName, prLevel, clouds, username, password, directory)\n",
    "    delete(os.path.join(directory,'w'))\n",
    "    unzip(directory)\n",
    "    buildCube(directory, resolution, clouds, plName, prLevel)\n",
    "    merge_Sentinel(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################SST###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(year, directorySST):\n",
    "    '''\n",
    "    Downloads the sst data file for the given year\n",
    "    \n",
    "    Parameters:\n",
    "        year (int): The year the sst is needed\n",
    "        directorySST (str): Pathlike string to the directory\n",
    "   '''\n",
    "    \n",
    "    start = datetime.now()\n",
    "    ftp = FTP('ftp.cdc.noaa.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/Projects/Datasets/noaa.oisst.v2.highres/')\n",
    "\n",
    "    files = ftp.nlst()\n",
    "    counter = 0\n",
    "\n",
    "    for file in files:\n",
    "        if file == 'sst.day.mean.' + str(year) + '.nc':\n",
    "            print(\"Downloading... \" + file)\n",
    "            ftp.retrbinary(\"RETR \" + file, open(directorySST + file, 'wb').write)      \n",
    "            ftp.close()\n",
    "            end = datetime.now()\n",
    "            diff = end - start\n",
    "            print('File downloaded ' + str(diff.seconds) + 's')\n",
    "        else: counter += 1\n",
    "    \n",
    "        if counter == len(files):\n",
    "            print('No matching dataset found for this year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datacubes(ds_merge):\n",
    "    '''\n",
    "    Merges datacubes by coordinates\n",
    "    \n",
    "    Parameters:\n",
    "        ds_merge (xArray Dataset[]): Array of datasets to be merged\n",
    "        \n",
    "    Returns: \n",
    "        ds1 (xArray Dataset): A single datacube with all merged datacubes\n",
    "    '''\n",
    "    \n",
    "    start = datetime.now()\n",
    "    if len(ds_merge) == 0:\n",
    "        print(\"Error: No datacubes to merge\")\n",
    "        return\n",
    "    if len(ds_merge) == 1:\n",
    "        return ds_merge[0]\n",
    "    else:\n",
    "        print('Start merging')\n",
    "        ds1 = ds_merge[0]\n",
    "        count = 1\n",
    "        while count < len(ds_merge):\n",
    "            start1 = datetime.now()\n",
    "            print(ds_merge)\n",
    "            ds1 =  xr.combine_by_coords([ds1, ds_merge[count]], combine_attrs=\"override\")\n",
    "            count += 1\n",
    "            diff = datetime.now() - start1\n",
    "            print(\"Succesfully merged cube nr \" + str(count) + \" to the base cube in \"+ str(diff.seconds) + 's')\n",
    "        diff = datetime.now() - start\n",
    "        print('All cubes merged for ' + str(diff.seconds) + 's')\n",
    "        return ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_datacubeSST(ds, name, directorySST):\n",
    "    '''\n",
    "    Saves the Datacube as NetCDF (.nc)\n",
    "      \n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Sourcedataset\n",
    "        name (str): Name or timeframe for saving eg '2017', '2015_2019'\n",
    "        directorySST (str): Pathlike string to the directory\n",
    "    '''\n",
    "    \n",
    "    print(\"Start saving\")\n",
    "    start = datetime.now()\n",
    "    if type(name) != str:\n",
    "        name = str(name)\n",
    "    ds.to_netcdf(directorySST + \"sst.day.mean.\" + name + \".nc\")\n",
    "    diff = datetime.now() - start\n",
    "    print(\"Done saving after \"+ str(diff.seconds) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainSST(yearBegin, yearEnd, directorySST, name):\n",
    "    '''\n",
    "    The main function to download, merge and safe the datacubes\n",
    "\n",
    "    Parameters:\n",
    "        yearBegin (int): First year to download\n",
    "        yearEnd (int): Last year to download\n",
    "        directorySST (str): Pathlike string to the directory\n",
    "        name (str): Name or timeframe for saving eg 'datacube', '2015_2019'\n",
    "    '''\n",
    "\n",
    "    if yearBegin > yearEnd:\n",
    "        print(\"Wrong years\")\n",
    "    else:\n",
    "        i = yearBegin\n",
    "        j = 0\n",
    "        while i <= yearEnd:\n",
    "#             download_file(i, directorySST)\n",
    "            i = i + 1\n",
    "        if yearBegin == yearEnd:\n",
    "            os.rename(os.path.join(directorySST, os.listdir(directorySST)[0]),directorySST + \"sst.day.mean.\" + name + \".nc\")\n",
    "        else:\n",
    "            ds_merge = []\n",
    "            for filename in os.listdir(directorySST):\n",
    "                cube = xr.open_dataset(os.path.join(directorySST, filename))\n",
    "                ds_merge.append(cube)\n",
    "                j = j + 1\n",
    "            datacube = merge_datacubes(ds_merge)\n",
    "            safe_datacubeSST(datacube, name, directorySST)\n",
    "            datacube.close()\n",
    "            for file in ds_merge:\n",
    "                file.close()\n",
    "            for file in os.listdir(directorySST):\n",
    "                if file == \"sst.day.mean.\" + name + \".nc\":\n",
    "                    continue\n",
    "                else:\n",
    "                    delete(os.path.join(directorySST, file))\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################Wrapper#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_collection(collection, params):\n",
    "    '''\n",
    "    Executes the SST - or the Sentinel - Dataprocess\n",
    "    \n",
    "    Parameters:\n",
    "        collection (str): The collection which is needed, SST or Sentinel2\n",
    "        params ([]): The params for executing the main - method\n",
    "   '''\n",
    "        \n",
    "    if collection == \"SST\":\n",
    "        yearBegin = params[0]\n",
    "        yearEnd = params[1]\n",
    "        directorySST = params[2]\n",
    "        name = params[3]\n",
    "        mainSST(yearBegin, yearEnd, directorySST, name)\n",
    "        \n",
    "    elif collection == \"Sentinel2\":\n",
    "        resolution = 100\n",
    "        directory = params[0]\n",
    "        collectionDate = params[1]\n",
    "        clouds = params[2]\n",
    "        username = params[3]\n",
    "        password = params[4]\n",
    "        aoi = 'POLYGON((7.52834379254901 52.01238155392252,7.71417925515199 52.01183230436206,7.705255583805303 51.9153349236737,7.521204845259327 51.90983021961716,7.52834379254901 52.01238155392252,7.52834379254901 52.01238155392252))'\n",
    "        mainSentinel(resolution, directory, collectionDate, aoi, clouds, username, password)\n",
    "    \n",
    "    else:\n",
    "        raise NameError(\"No Collection named like this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# paramsSentinel = ['D:/Tatjana/Documents/Studium/Semester 5 - Abgaben/Geosoftware 2/Code/Sentinel_Data/', ('20200601', '20200615'), (0, 30), \"\", \"\"]\n",
    "# load_collection(\"Sentinel2\", paramsSentinel)\n",
    "\n",
    "# paramsSST = [2013, 2015, 'C:/Users/adria/Desktop/Uni/Semester5/Geosoft2/Github/GeoSoftII_DataServer/Sentinel/Data/', 'datacube']\n",
    "# load_collection(\"SST\", paramsSST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
