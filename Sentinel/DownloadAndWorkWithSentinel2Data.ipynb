{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "    - Tests\n",
    "        - Fehlermedlung der API einbauen\n",
    "    - STAC\n",
    "        - Dask\n",
    "    - Mergen testen\n",
    "    - Slicen testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import geopandas as gpd\n",
    "import getpass\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from time import sleep\n",
    "import stat\n",
    "import io\n",
    "from rasterio.enums import Resampling\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadingData (aoi, collectionDate, plName, prLevel, clouds,username,password):\n",
    "    '''\n",
    "    Downloads the Sentinel2-Data with the given parameters\n",
    "    \n",
    "    Parameter: \n",
    "        aoi(str) = The type and the coordinates of the area of interest\n",
    "        collectionDate = The date of the data\n",
    "        plName(str) = The name of the platform\n",
    "        prLevel(str) = The name of the process\n",
    "        clouds = The allowed percentage of the cloudcover\n",
    "    '''\n",
    "    api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')\n",
    "    \n",
    "    '''Choosing the data with bounding box (footprint), date, platformname, processinglevel and cloudcoverpercentage'''\n",
    "    products = api.query(aoi, date = collectionDate, platformname = plName, processinglevel = prLevel, cloudcoverpercentage = clouds)\n",
    "    \n",
    "    '''Filters the products and sort the by cloudcoverpercentage'''\n",
    "    products_gdf = api.to_geodataframe(products)\n",
    "    products_gdf_sorted = products_gdf.sort_values(['cloudcoverpercentage'], ascending=[True])\n",
    "    \n",
    "    '''Downloads the choosen files from Scihub'''\n",
    "    #saveFile(products_gdf_sorted)\n",
    "\n",
    "    products_gdf_sorted.to_csv('w')\n",
    "    api.download_all(products,directory, max_attempts=10, checksum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listDir (path):\n",
    "    '''\n",
    "    Lists all files from the given directory\n",
    "    \n",
    "    Parameter: \n",
    "        path(str): Path to the directory\n",
    "        \n",
    "    Returns:\n",
    "        path(str[]): An array of all filenames\n",
    "    '''\n",
    "    return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "def unziping (filename):\n",
    "    '''\n",
    "    Unzips the file with the given filename\n",
    "    \n",
    "    Parameter: \n",
    "        filename(str): Name of the .zip file\n",
    "    '''\n",
    "    with ZipFile(os.path.join(directory, filename), 'r') as zipObj:\n",
    "        zipObj.extractall(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete (path):\n",
    "    '''\n",
    "    Deletes the file/directory with the given path\n",
    "    \n",
    "    Parameter: \n",
    "        path(str): Path to the file/directory\n",
    "    '''\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(\"File deleted: \" + path)\n",
    "    else:\n",
    "        print(\"The file does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractBands (filename,resolution):\n",
    "    '''\n",
    "    Extracts bandpaths from .SAFE file\n",
    "    \n",
    "    Parameter: \n",
    "        filename(str): Sentinel .SAFE file\n",
    "        \n",
    "    Returns:\n",
    "        bandPaths(str[]): An array of the paths for the red and nir band\n",
    "    '''\n",
    "    lTwoA = listDir(os.path.join(directory, filename, \"GRANULE\"))\n",
    "    \n",
    "    if resolution == 60:\n",
    "        bandName = listDir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[4]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[11]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "    \n",
    "    elif resolution == 20:   \n",
    "        bandName = listDir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "    \n",
    "    elif resolution == 10:\n",
    "        bandName = listDir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[4]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "        \n",
    "    elif resolution == 100:\n",
    "        bandName = listDir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "        pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "        pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "        bandPaths = [pathRed, pathNIR]\n",
    "    else:\n",
    "        print(\"no such resolution\")\n",
    "        return -1\n",
    "    \n",
    "    return bandPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBand (bandpath,date,tile,resolution, clouds, plName, prLevel):\n",
    "    '''\n",
    "    Opens and reads the red and nir band, saves them as NetCDF file\n",
    "    \n",
    "    Parameter: \n",
    "        bandPaths(str[]): Array with the paths to the red and nir band\n",
    "    '''\n",
    "    b4 = rio.open(bandpath[0])\n",
    "    b8 = rio.open(bandpath[1])\n",
    "    red = b4.read()\n",
    "    nir = b8.read()    \n",
    "\n",
    "    if resolution==10:\n",
    "        res=1830*3*2\n",
    "    elif resolution == 20:\n",
    "        res = 1830*3\n",
    "    elif resolution == 60:\n",
    "        res = 1830\n",
    "    elif resolution == 100:\n",
    "        res = 1098\n",
    "    else:\n",
    "        print(\"No such resolution\")\n",
    "        return -1\n",
    "    \n",
    "    j=res-1\n",
    "    i=0\n",
    "    lat = [0]*res\n",
    "    lon = [0]*res\n",
    "    while j>=0:\n",
    "        lon[i]=b4.bounds.left + i*resolution \n",
    "        lat[i]=b4.bounds.bottom + j*resolution\n",
    "        i=i+1\n",
    "        j=j-1\n",
    "\n",
    "    time = pd.date_range(date, periods=1)\n",
    "    \n",
    "    \n",
    "    if resolution == 100:\n",
    "        upscale_factor = (1/5) \n",
    "        # resample data to target shape\n",
    "        nir = b8.read(\n",
    "               out_shape=(\n",
    "                   b8.count,\n",
    "                   int(b8.height * upscale_factor),\n",
    "                   int(b8.width * upscale_factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear)\n",
    "            # scale image transform\n",
    "        transform = b8.transform * b8.transform.scale(\n",
    "                (b8.width / nir.shape[-1]),\n",
    "                (b8.height / nir.shape[-2])\n",
    "            )\n",
    "\n",
    "        red = b4.read(\n",
    "               out_shape=(\n",
    "                   b4.count,\n",
    "                   int(b4.height * upscale_factor),\n",
    "                   int(b4.width * upscale_factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "            # scale image transform\n",
    "        transform = b4.transform * b4.transform.scale(\n",
    "                (b4.width / red.shape[-1]),\n",
    "                (b4.height / red.shape[-2])\n",
    "            )\n",
    "    \n",
    "    dataset = xarray.Dataset(\n",
    "          {\n",
    "        \"red\": ([\"time\",\"lat\", \"lon\"], red),\n",
    "        \"nir\": ([\"time\",\"lat\", \"lon\"], nir)\n",
    "        },\n",
    "         coords=dict( \n",
    "            time=time,\n",
    "            lat=([\"lat\"], lat),\n",
    "            lon=([\"lon\"], lon),\n",
    "        ),\n",
    "         attrs=dict(       \n",
    "             platform= plName,\n",
    "             processingLevel= prLevel,\n",
    "             cloudcover = clouds,\n",
    "             source = \"https://scihub.copernicus.eu/dhus\",\n",
    "             resolution= str(resolution) +\" x \"+ str(resolution)+\" m\" \n",
    "         ),\n",
    "    )\n",
    "  \n",
    "\n",
    "    dataset.to_netcdf(directory+\"datacube_\"+str(date)+\"_\"+str(tile)+\"_R\"+str(resolution)+\".nc\", 'w', format='NETCDF4')\n",
    "    b4.close()\n",
    "    b8.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(filename):\n",
    "    '''\n",
    "    extracts the Date of the Sentinelfilename\n",
    "    Parameters:\n",
    "        filename (str): name of the file\n",
    "    Returns: \n",
    "        (str): Date of the File (\"2020-12-31\")\n",
    "    '''\n",
    "    return filename[11:15]+\"-\"+filename[15:17]+\"-\"+filename[17:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTile(filename):\n",
    "    '''\n",
    "    extracts the UTM-tile of the Sentinelfilename\n",
    "    Parameters:\n",
    "        filename (str): name of the file\n",
    "    Returns: \n",
    "        (str): UTM-tile of the File (\"31UMC\")\n",
    "    '''\n",
    "    return filename[38:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_rm_error( func, path, exc_info):\n",
    "       # path contains the path of the file that couldn't be removed\n",
    "       # let's just assume that it's read-only and unlink it.\n",
    "       os.chmod( path, stat.S_IWRITE )\n",
    "       os.unlink( path )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(directory, deleteZip):    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".zip\"): \n",
    "            unziping(filename)\n",
    "            if deleteZip == True:\n",
    "                delete(os.path.join(directory, filename))\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildCube(directory, deleteSafe, resolution, clouds, plName, prLevel):    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".SAFE\"): \n",
    "            bandPath = extractBands(os.path.join(directory, filename),resolution)\n",
    "            band = loadBand(bandPath,getDate(filename),getTile(filename),resolution, clouds, plName, prLevel)\n",
    "            if deleteSafe==True:\n",
    "                shutil.rmtree(os.path.join(directory, filename) , onerror = on_rm_error)\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "            print(\" \")\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(resolution, directory, collectionDate, aoi, clouds, username, password, deleteZip, deleteSafe):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    plName = 'Sentinel-2'\n",
    "    prLevel = 'Level-2A'\n",
    "#     downloadingData (aoi, collectionDate, plName, prLevel, clouds, username, password)\n",
    "    unzip(directory, deleteZip)\n",
    "    buildCube(directory, deleteSafe, resolution, clouds, plName, prLevel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/adria/Desktop/Uni/Semester5/Geosoft2/Code/Notebooks/Sentinel/Data/'\n",
    "resolution = 100    #10, 20, 60, 100 möglich\n",
    "\n",
    "'''Parameters for the download'''\n",
    "aoi = 'POLYGON((7.52834379254901 52.01238155392252,7.71417925515199 52.01183230436206,7.705255583805303 51.9153349236737,7.521204845259327 51.90983021961716,7.52834379254901 52.01238155392252,7.52834379254901 52.01238155392252))'\n",
    "collectionDate = ('20200601', '20200610')\n",
    "clouds = (0,30)\n",
    "\n",
    "\n",
    "main( resolution, directory, collectionDate, aoi, clouds, username, password, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: ········\n",
      "password: ········\n"
     ]
    }
   ],
   "source": [
    "username = getpass.getpass(\"user: \")\n",
    "password = getpass.getpass(\"password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def merge_datacubes(ds_merge):\n",
    "    '''\n",
    "    merges datacubes by coordinates\n",
    "    \n",
    "    Parameters:\n",
    "        ds_merge (array): array of datasets to be mearched\n",
    "        \n",
    "    Returns: \n",
    "        ds1 (ds): A single datacube with all merged datacubes\n",
    "        - Error, if no Datacubes given\n",
    "    '''\n",
    "    start = datetime.now()\n",
    "    if len(ds_merge) == 0:\n",
    "        print(\"error\")\n",
    "        return\n",
    "    if len(ds_merge) == 1:\n",
    "        return ds_merge[0]\n",
    "    else:\n",
    "        print('start merging')\n",
    "        ds1 = ds_merge[0]\n",
    "        count = 1\n",
    "        while count < len(ds_merge):\n",
    "            start1 = datetime.now()\n",
    "            ds1 =  xr.combine_by_coords([ds1,ds_merge[count]],compat=\"override\")\n",
    "            count+=1\n",
    "            print(\"succesfully merged cube nr \"+ str(count)+\" to the base cube \")\n",
    "            end = datetime.now()\n",
    "            diff = end - start1\n",
    "            print('All cubes merged for ' + str(diff.seconds) + 's')\n",
    "        print(\"result: \")\n",
    "        print(ds1)\n",
    "        end = datetime.now()\n",
    "        diff = end - start\n",
    "        print('All cubes merged for ' + str(diff.seconds) + 's')\n",
    "        return ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeframe(ds,start,end):\n",
    "    '''\n",
    "    Slices Datacube down to given timeframe\n",
    "      \n",
    "    Parameters:\n",
    "        ds (ds): source dataset\n",
    "        start (str): start of the timeframe eg '2018-07-13'\n",
    "        end (str): end of the timeframe eg '2018-08-23'\n",
    "       \n",
    "    Returns:\n",
    "        ds_selected (ds): dataset sliced to timeframe\n",
    "    '''\n",
    "    if start>end:\n",
    "        print(\"start and end of the timeframe do are not compatible!\")\n",
    "    else:    \n",
    "        ds_selected = ds.sel(time = slice(start,end))\n",
    "        #print(ds_selected)\n",
    "        return ds_selected    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_datacube(ds, timeframe):\n",
    "    '''\n",
    "    Saves the Datacube as NetCDF (.nc)\n",
    "      \n",
    "    Parameters:\n",
    "        ds (ds): source dataset\n",
    "        timeframe (str): timeframe eg '2017', '2015_2019'\n",
    "    '''\n",
    "    print(\"start saving\")\n",
    "    if type(timeframe) != str:\n",
    "        timeframe=str(timeframe)\n",
    "    ds.to_netcdf(systempath+ \"sst.day.mean.\"+timeframe+\".nc\")\n",
    "    print(\"done saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 1098, lon: 1098, time: 1)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2020-06-01\n",
      "  * lat      (lat) float64 5.8e+06 5.8e+06 5.8e+06 ... 5.69e+06 5.69e+06\n",
      "  * lon      (lon) float64 3e+05 3.001e+05 3.002e+05 ... 4.096e+05 4.097e+05\n",
      "Data variables:\n",
      "    red      (time, lat, lon) uint16 ...\n",
      "    nir      (time, lat, lon) uint16 ...\n",
      "Attributes:\n",
      "    platform:         Sentinel-2\n",
      "    processingLevel:  Level-2A\n",
      "    cloudcover:       [ 0 30]\n",
      "    source:           https://scihub.copernicus.eu/dhus\n",
      "    resolution:       100 x 100 m\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 1098, lon: 1098, time: 1)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2020-06-01\n",
      "  * lat      (lat) float64 5.8e+06 5.8e+06 5.8e+06 ... 5.69e+06 5.69e+06\n",
      "  * lon      (lon) float64 4e+05 4.001e+05 4.002e+05 ... 5.096e+05 5.097e+05\n",
      "Data variables:\n",
      "    red      (time, lat, lon) uint16 ...\n",
      "    nir      (time, lat, lon) uint16 ...\n",
      "Attributes:\n",
      "    platform:         Sentinel-2\n",
      "    processingLevel:  Level-2A\n",
      "    cloudcover:       [ 0 30]\n",
      "    source:           https://scihub.copernicus.eu/dhus\n",
      "    resolution:       100 x 100 m\n",
      "start merging\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot specify both coords='different' and compat='override'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-b4712dd96577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_2019\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mds_merge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mds_2018\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mds_2019\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mds_2018_2019\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_datacubes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_merge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_2018_2019\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#ds_2018_2019 = xr.open_dataset(systempath+\"sst.day.mean.2018-2019.nc\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-85acbe7a31d1>\u001b[0m in \u001b[0;36mmerge_datacubes\u001b[1;34m(ds_merge)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_merge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mstart1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mds1\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_by_coords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mds1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mds_merge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"override\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"succesfully merged cube nr \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" to the base cube \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36mcombine_by_coords\u001b[1;34m(datasets, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Concatenate along all of concat_dims one by one to create single ds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         concatenated = _combine_nd(\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[0mcombined_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mconcat_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_nd\u001b[1;34m(combined_ids, concat_dims, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# element of the tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mconcat_dim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconcat_dims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         combined_ids = _combine_all_along_first_dim(\n\u001b[0m\u001b[0;32m    199\u001b[0m             \u001b[0mcombined_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_all_along_first_dim\u001b[1;34m(combined_ids, dim, data_vars, coords, compat, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mcombined_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         new_combined_ids[new_id] = _combine_1d(\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\combine.py\u001b[0m in \u001b[0;36m_combine_1d\u001b[1;34m(datasets, concat_dim, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconcat_dim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m             combined = concat(\n\u001b[0m\u001b[0;32m    258\u001b[0m                 \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;34m\"objects, got %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         )\n\u001b[1;32m--> 191\u001b[1;33m     return f(\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombine_attrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36m_dataset_concat\u001b[1;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# determine which variables to concatentate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     concat_over, equals, concat_dim_lengths = _calc_concat_over(\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36m_calc_concat_over\u001b[1;34m(datasets, dim, dim_names, data_vars, coords, compat)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[0mprocess_subset_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_vars\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m     \u001b[0mprocess_subset_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coords\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcat_over\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mequals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_dim_lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\core\\concat.py\u001b[0m in \u001b[0;36mprocess_subset_opt\u001b[1;34m(opt, subset)\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"different\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcompat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"override\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    248\u001b[0m                         \u001b[1;34m\"Cannot specify both %s='different' and compat='override'.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                         \u001b[1;33m%\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot specify both coords='different' and compat='override'."
     ]
    }
   ],
   "source": [
    "ds_2018 = xr.open_dataset(directory+\"datacube_2020-06-01_T32ULC_R100.nc\")\n",
    "ds_2019 = xr.open_dataset(directory+\"datacube_2020-06-01_T32UMC_R100.nc\")\n",
    "print(ds_2018)\n",
    "print(ds_2019)\n",
    "ds_merge = [ds_2018,ds_2019]\n",
    "ds_2018_2019 = merge_datacubes(ds_merge)\n",
    "print(ds_2018_2019)\n",
    "#ds_2018_2019 = xr.open_dataset(systempath+\"sst.day.mean.2018-2019.nc\")\n",
    "# ds_sliced = timeframe(ds_2018_2019,'2018-07-01','2019-08-01')\n",
    "# print(ds_sliced.time)\n",
    "# safe_datacube(ds_2018_2019,\"201819\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
