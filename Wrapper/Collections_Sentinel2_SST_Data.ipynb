{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@author Adrian Spork https://github.com/A-Spork\n",
    "#@author Tatjana Melina Walter https://github.com/jana2308walter\n",
    "#@author Maximilian Busch https://github.com/mabu1994\n",
    "#@author digilog11 https://github.com/digilog11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import getpass\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import stat\n",
    "from rasterio.enums import Resampling\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from ftplib import FTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dask Cluster'''\n",
    "from dask.distributed import Client, LocalCluster\n",
    "'''Python 3.9.1 Workaround'''\n",
    "# # import multiprocessing.popen_spawn_posix#nonwindows#\n",
    "# import multiprocessing.popen_spawn_win32#windows#\n",
    "# from distributed import Client#\n",
    "# Client()#\n",
    "'''Server'''\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPath(Exception):\n",
    "    def init(self, message):\n",
    "        self.message = message\n",
    "    pass\n",
    "\n",
    "class NoResolution(Exception):\n",
    "    def init(self, message):\n",
    "        self.message = message\n",
    "    pass\n",
    "\n",
    "class NoSafeFileError(Exception):\n",
    "    def init(self,message):\n",
    "        self.message = message\n",
    "    pass \n",
    "\n",
    "class FileNotFoundError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "    \n",
    "class DirectoryNotFoundError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "\n",
    "class TimeframeError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "    \n",
    "class NotNetCDFError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "    \n",
    "class FilenameError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "    \n",
    "class TimeframeLengthError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "\n",
    "class TimeframeValueError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message\n",
    "    \n",
    "class ParameterTypeError(Exception):\n",
    "  def __init__(self, message):\n",
    "    self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadingData(aoi, collectionDate, plName, prLevel, clouds, username, password, directory):\n",
    "    '''\n",
    "    Downloads the Sentinel2 - Data with the given parameters\n",
    "\n",
    "    Parameters:\n",
    "        aoi (str): The type and the coordinates of the area of interest\n",
    "        collectionDate datetime 64[ns]): The date of the data\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The name of the process\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        username (str): The username of the Copernicus SciHub\n",
    "        password (str): The password of the Copernicus SciHub\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "    \n",
    "    api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')\n",
    "    \n",
    "    '''Choosing the data with bounding box (footprint), date, platformname, processinglevel and cloudcoverpercentage'''\n",
    "    products = api.query(aoi, date = collectionDate, platformname = plName, processinglevel = prLevel, cloudcoverpercentage = clouds)\n",
    "\n",
    "    '''Downloads the choosen files from Scihub'''\n",
    "    if len(products)==0:\n",
    "        raise Exception(\"No data for this params\")\n",
    "    print(\"Start downloading \" + str(len(products)) + \" product(s)\")\n",
    "    api.download_all(products, directory, max_attempts = 10, checksum = True)\n",
    "    print(\"All necassary downloads done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzipping(filename, directory):\n",
    "    '''\n",
    "    Unzips the file with the given filename\n",
    "    Parameter:\n",
    "        filename(str): Name of the .zip file\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "    with ZipFile(os.path.join(directory, filename), 'r') as zipObj:\n",
    "        zipObj.extractall(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(directory):\n",
    "    '''\n",
    "    Unzips and deletes the .zip in the given directory\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".zip\"):\n",
    "            if(filename[39:41]!=\"32\"):\n",
    "                print(\"CRS not supported! Only EPSG:32632 supported\")\n",
    "                delete(os.path.join(directory,filename))\n",
    "            else:\n",
    "                unzipping(filename, directory)\n",
    "                delete(os.path.join(directory, filename))\n",
    "                continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractBands(filename, resolution, directory):\n",
    "    '''\n",
    "    Extracts bandpaths from the given .SAFE file\n",
    "    Parameters:\n",
    "        filename (str): Sentinel .SAFE file\n",
    "        resolution (int): The resolution the datacube should have\n",
    "        directory (str): Pathlike string to the directory\n",
    "    Returns:\n",
    "        bandPaths (str[]): An array of the paths for the red and nir band\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        lTwoA = os.listdir(os.path.join(directory, filename, \"GRANULE\"))\n",
    "        if resolution == 10:\n",
    "            bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\"))\n",
    "            pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[3]))\n",
    "            pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R10m\", str(bandName[4]))\n",
    "            bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "        elif resolution == 20:\n",
    "            bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "            pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "            pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "            bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "        elif resolution == 60:\n",
    "            bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\"))\n",
    "            pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[4]))\n",
    "            pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R60m\", str(bandName[11]))\n",
    "            bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "        elif resolution == 100:\n",
    "            bandName = os.listdir (os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\"))\n",
    "            pathRed = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[3]))\n",
    "            pathNIR = os.path.join(directory, filename, \"GRANULE\", str(lTwoA[0]), \"IMG_DATA\", \"R20m\", str(bandName[9]))\n",
    "            bandPaths = [pathRed, pathNIR]\n",
    "\n",
    "        else:\n",
    "               raise NoResolution(\"Invalid Resolution, try 10, 20, 60 or 100\")\n",
    "    except FileNotFoundError:\n",
    "        raise NoPath(\"No file in this path\")\n",
    "    return bandPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBand (bandpath, date, tile, resolution, clouds, plName, prLevel, directory):\n",
    "    '''\n",
    "    Opens and reads the red and nir band, saves them as NetCDF file\n",
    "    Parameters:\n",
    "        bandPaths (str[]): Array with the paths to the red and nir band\n",
    "        date (datetime 64[ns]): The collection date (\"2020-12-31\")\n",
    "        tile (str): Bounding box of coordinates defined by Sentinel\n",
    "        resolution (int): The resolution of the dataset\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The level of the process\n",
    "        directory (str): Pathlike string to the directory\n",
    "    Returns:\n",
    "        dataset (xArray dataset): The result dataset as xArray dataset\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    b4 = rio.open(bandpath[0])\n",
    "    b8 = rio.open(bandpath[1])\n",
    "    red = b4.read()\n",
    "    nir = b8.read()\n",
    "\n",
    "    if resolution == 10:\n",
    "        res = 1830 * 3 * 2\n",
    "    elif resolution == 20:\n",
    "        res = 1830 * 3\n",
    "    elif resolution == 60:\n",
    "        res = 1830\n",
    "    elif resolution == 100:\n",
    "        res = 1098\n",
    "    else:\n",
    "        raise NoResolution(\"Invalid Resolution, try 10, 20, 60 or 100\")\n",
    "\n",
    "    j = res - 1\n",
    "    i = 0\n",
    "    lat = [0] * res\n",
    "    lon = [0] * res\n",
    "    while j >= 0:\n",
    "        lon[i] = b4.bounds.left + i * resolution\n",
    "        lat[i] = b4.bounds.bottom + j * resolution\n",
    "        i = i + 1\n",
    "        j = j - 1\n",
    "\n",
    "    time = pd.date_range(date, periods = 1)\n",
    "\n",
    "    if resolution == 100:\n",
    "        upscale_factor = (1/5)\n",
    "        nir = b8.read(\n",
    "                out_shape = (\n",
    "                    b8.count,\n",
    "                    int(b8.height * upscale_factor),\n",
    "                    int(b8.width * upscale_factor)\n",
    "                ),\n",
    "                resampling = Resampling.bilinear\n",
    "        )\n",
    "        transform = b8.transform * b8.transform.scale(\n",
    "            (b8.width / nir.shape[-1]),\n",
    "            (b8.height / nir.shape[-2])\n",
    "        )\n",
    "        red = b4.read(\n",
    "            out_shape = (\n",
    "                b4.count,\n",
    "                int(b4.height * upscale_factor),\n",
    "                int(b4.width * upscale_factor)\n",
    "            ),\n",
    "            resampling = Resampling.bilinear\n",
    "        )\n",
    "\n",
    "        transform = b4.transform * b4.transform.scale(\n",
    "            (b4.width / red.shape[-1]),\n",
    "            (b4.height / red.shape[-2])\n",
    "        )\n",
    "\n",
    "    dataset = xr.Dataset(\n",
    "        {\n",
    "            \"red\": ([\"time\",\"lat\", \"lon\"], red),\n",
    "            \"nir\": ([\"time\",\"lat\", \"lon\"], nir)\n",
    "        },\n",
    "        coords = dict(\n",
    "            time = time,\n",
    "            lat = ([\"lat\"], lat),\n",
    "            lon = ([\"lon\"], lon),\n",
    "        ),\n",
    "        attrs = dict(\n",
    "            platform = plName,\n",
    "            processingLevel = prLevel,\n",
    "            source = \"https://scihub.copernicus.eu/dhus\",\n",
    "            resolution = str(resolution) + \" x \" + str(resolution) + \" m\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dataset.to_netcdf(directory + \"datacube_\" + str(date) + \"_\" + str(tile) + \"_R\" + str(resolution) + \".nc\", 'w', format = 'NETCDF4')\n",
    "    b4.close()\n",
    "    b8.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDate(filename):\n",
    "    '''\n",
    "    Extracts the Date out of the Sentinelfilename\n",
    "    Parameters:\n",
    "        filename (str): Name of the file\n",
    "    Returns:\n",
    "        (str): Date of the File (\"2020-12-31\")\n",
    "    '''\n",
    "\n",
    "    return filename[11:15] + \"-\" + filename[15:17] + \"-\" + filename[17:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTile(filename):\n",
    "    '''\n",
    "    Extracts the UTM-tile of the Sentinelfilename\n",
    "    Parameters:\n",
    "        filename (str): Name of the file\n",
    "    Returns:\n",
    "        (str): UTM-tile of the File (\"31UMC\")\n",
    "    '''\n",
    "    return filename[38:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_rm_error(func, path, exc_info):\n",
    "    '''\n",
    "    Unlinks a read-only file\n",
    "    '''\n",
    "\n",
    "    os.chmod(path, stat.S_IWRITE)\n",
    "    os.unlink(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCube(directory, resolution, clouds, plName, prLevel):\n",
    "    '''\n",
    "    Builds a datacube in the given directory with coords, time as dimensions and the bands as datavariables\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string to the directory\n",
    "        resolution (int): The resolution of the dataset\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        plName (str): The name of the platform\n",
    "        prLevel (str): The level of the process\n",
    "    '''\n",
    "    \n",
    "    i = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".SAFE\"):\n",
    "            i = i + 1\n",
    "    if i == 0:\n",
    "        raise NoSafeFileError (\"In this directory is no SAFE file to build a cube\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".SAFE\"):\n",
    "            bandPath = extractBands(os.path.join(directory, filename), resolution, directory)\n",
    "            band = loadBand(bandPath, getDate(filename), getTile(filename), resolution, clouds, plName, prLevel, directory)\n",
    "            shutil.rmtree(os.path.join(directory, filename), onerror = on_rm_error)\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Sentinel(directory, nameSentinel):\n",
    "    '''\n",
    "    Merges datacubes by coordinates and time\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Pathlike string where Data is stored\n",
    "        nameSentinel (str): Filename for the datacube\n",
    "    '''\n",
    "\n",
    "    start = datetime.now()\n",
    "    files = os.listdir(directory)\n",
    "    for nc in files:\n",
    "        if not nc.endswith(\".nc\"):\n",
    "            raise TypeError(\"Wrong file in directory\")\n",
    "    if len(files) == 0:\n",
    "        raise FileNotFoundError(\"Directory empty\")\n",
    "    elif len(files) == 1:\n",
    "        print(\"Only one file in directory\")\n",
    "        os.rename(directory + (os.listdir(directory)[0]), directory + \"merged_cube.nc\")\n",
    "        return\n",
    "    else:\n",
    "        print('Start merging')\n",
    "        for file1 in files:\n",
    "            for file2 in files:\n",
    "                file1Date = file1[9:19]\n",
    "                file1Tile = file1[20:26]\n",
    "                file1Res = file1[27:31]\n",
    "                file2Date = file2[9:19]\n",
    "                file2Tile = file2[20:26]\n",
    "                file2Res = file2[27:31]\n",
    "                if file1[21:23] == \"31\":\n",
    "                    delete(os.path.join(directory,file1))\n",
    "                elif file2[21:23] == \"31\":\n",
    "                    delete(os.path.join(directory,file2))\n",
    "                elif file1Date == file2Date and file1Tile == file2Tile and file1Res == file2Res:\n",
    "                    continue\n",
    "                elif file1Date == file2Date and file1Tile == \"T32ULC\" and file2Tile == \"T32UMC\" and file1Res == file2Res:\n",
    "                    fileLeft = xr.open_dataset(os.path.join(directory, file1))\n",
    "                    fileRight = xr.open_dataset(os.path.join(directory, file2))\n",
    "                    merge_coords(fileLeft, fileRight, file1[0:20] + \"Merged\" + file1[26:31], directory)\n",
    "                    fileLeft.close()\n",
    "                    fileRight.close()\n",
    "                    delete(os.path.join(directory, file1))\n",
    "                    delete(os.path.join(directory, file2))                   \n",
    "\n",
    "    ds_merge = []\n",
    "    files = []\n",
    "    for f in os.listdir(directory):\n",
    "        x = xr.open_dataset(os.path.join(directory, f))\n",
    "        ds_merge.append(x)\n",
    "        files.append(os.path.join(directory, f))\n",
    "#    datacube = xr.open_mfdataset(files)'''none dask'''\n",
    "    datacube = xr.open_mfdataset(files, parallel=True, chunks={\"time\": \"auto\"})'''with dask'''\n",
    "    '''save datacube'''\n",
    "    print(\"Start saving\")\n",
    "    datacube.to_netcdf(directory + nameSentinel + \".nc\", compute = True)\n",
    "    print(\"Done saving\")\n",
    "    datacube.close()\n",
    "\n",
    "    for f in ds_merge:\n",
    "        f.close()\n",
    "    for f in files:\n",
    "        deleteNetcdf(f)\n",
    "        \n",
    "    end = datetime.now()\n",
    "    diff = end - start\n",
    "    print('All cubes merged for ' + str(diff.seconds) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeframe(ds, start, end):\n",
    "    '''\n",
    "    Slices Datacube down to given timeframe\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Sourcedataset\n",
    "        start (str): Start of the timeframe eg '2018-07-13'\n",
    "        end (str): End of the timeframe eg '2018-08-23'\n",
    "    Returns:\n",
    "        ds_selected (xArray Dataset): Dataset sliced to timeframe\n",
    "    '''\n",
    "\n",
    "    if start > end:\n",
    "        print(\"start and end of the timeframe are not compatible!\")\n",
    "    else:\n",
    "        ds_selected = ds.sel(time = slice(start, end))\n",
    "        return ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_datacube(ds, name, directory):\n",
    "    '''\n",
    "    Saves the Datacube as NetCDF (.nc)\n",
    "    Parameters:\n",
    "        ds (xArray Dataset): Sourcedataset\n",
    "        name (str): Name eg '2017', '2015_2019'\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    print(\"Start saving\")\n",
    "    start = datetime.now()\n",
    "    ds.to_netcdf(directory + name + \".nc\")\n",
    "    diff = datetime.now() - start\n",
    "    print(\"Done saving after \"+ str(diff.seconds) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_coords(ds_left, ds_right, name, directory):\n",
    "    '''\n",
    "    Merges two datasets by coordinates\n",
    "\n",
    "    Parameters:\n",
    "        ds_left (xArray dataset): Dataset to be merged\n",
    "        ds_right (xArray dataset): Dataset to be merged\n",
    "        name (str): Name of the new dataset\n",
    "        directory (str): Pathlike string to the directory\n",
    "    '''\n",
    "\n",
    "    ds_selected = ds_left.sel(lon = slice(ds_left.lon[0], ds_right.lon[0]))\n",
    "    ds_merge = [ds_selected, ds_right]\n",
    "    merged = xr.combine_by_coords(ds_merge)\n",
    "    safe_datacube(merged, name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def slice_lat(ds, lat_left, lat_right):\n",
    "#     '''\n",
    "#     Slices a given dataset to given latitude bounds\n",
    "#     Parameters:\n",
    "#         ds (xArray Dataset): Dataset to be sliced\n",
    "#         lat_left (float): Left latitude bound\n",
    "#         lat_right (float): Right latitude bound\n",
    "#     Returns:\n",
    "#         ds (xArray Dataset): Sliced dataset\n",
    "#     '''\n",
    "\n",
    "#     ds_selected = ds.sel(lat = slice(lat_left, lat_right))\n",
    "#     return ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def slice_lon(ds, lon_left, lon_right):\n",
    "#     '''\n",
    "#     Slices a given dataset to given longitude bounds\n",
    "#     Parameters:\n",
    "#         ds (xArray Dataset): Dataset to be sliced\n",
    "#         lon_left (float): Left longitude bound\n",
    "#         lon_right (float): Right longitude bound\n",
    "#     Returns:\n",
    "#         ds (xArray Dataset): Sliced dataset\n",
    "#     '''\n",
    "\n",
    "#     ds_selected = ds.sel(lon = slice(lon_left, lon_right))\n",
    "#     return ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def slice_coords(ds, lon_left, lon_right, lat_left, lat_right):\n",
    "#     '''\n",
    "#     Slices a dataset to a given slice\n",
    "#     Parameters:\n",
    "#         ds (xArray Dataset): Dataset to be sliced\n",
    "#         lon_left (float): Left bound for longitude\n",
    "#         lon_right (float): Right bound for longitude\n",
    "#         lat_left (float): Left bound for latitude\n",
    "#         lat_right (float): Right bound for latitude\n",
    "#     Returns:\n",
    "#         ds (xArray Dataset): Sliced dataset\n",
    "#     '''\n",
    "\n",
    "#     ds_selected = slice_lon(ds, lon_left, lon_right)\n",
    "#     return slice_lat(ds_selected, lat_left, lat_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(path):\n",
    "    '''\n",
    "    Deletes the file/directory with the given path\n",
    "    Parameters:\n",
    "        path (str): Path to the file/directory\n",
    "    '''\n",
    "    try: \n",
    "        os.remove(path)\n",
    "        print(\"File was deleted\")\n",
    "    except FileNotFoundError:\n",
    "        raise NoPath (\"No file in this path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainSentinel(resolution, directory, collectionDate, aoi, clouds, username, password, nameSentinel):\n",
    "    '''\n",
    "    Downloads, unzips, collects and merges Sentinel2 Satelliteimages to a single netCDF4 datacube\n",
    "\n",
    "    Parameters:\n",
    "        resolution (int): Resolution of the satelite image\n",
    "        directory (str): Pathlike string to the workdirectory\n",
    "        collectionDate (tuple of datetime 64[ns]): Start and end of the timeframe\n",
    "        aoi (POLYGON): Area of interest\n",
    "        clouds (tuple of ints): Min and max of cloudcoverpercentage\n",
    "        username (str): Uername for the Copernicus Open Acess Hub\n",
    "        password (str): Password for the Copernicus Open Acess Hub\n",
    "        nameSentinel (str): Filename for the datacube\n",
    "    '''\n",
    "    if collectionDate[0]==collectionDate[1]:\n",
    "        raise Exception(\"Start and end of collection can not be identical\")\n",
    "    plName = 'Sentinel-2'\n",
    "    prLevel = 'Level-2A'\n",
    "    downloadingData (aoi, collectionDate, plName, prLevel, clouds, username, password, directory)\n",
    "    unzip(directory)\n",
    "    buildCube(directory, resolution, clouds, plName, prLevel)\n",
    "    merge_Sentinel(directory, nameSentinel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(year, directory):\n",
    "    '''\n",
    "    Downloads the sst data file for the given year\n",
    "\n",
    "    Parameters:\n",
    "        year (int): year of the sst data\n",
    "        directory (str): path of future file\n",
    "    '''\n",
    "    \n",
    "    start = datetime.now()\n",
    "    ftp = FTP('ftp.cdc.noaa.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/Projects/Datasets/noaa.oisst.v2.highres/')\n",
    "\n",
    "    files = ftp.nlst()\n",
    "    counter = 0\n",
    "\n",
    "    for file in files:\n",
    "        if file == 'sst.day.mean.' + str(year) + '.nc':\n",
    "            print(\"Downloading...\" + file)\n",
    "            ftp.retrbinary(\"RETR \" + file, open(directory + file, 'wb').write)\n",
    "            ftp.close()\n",
    "            end = datetime.now()\n",
    "            diff = end - start\n",
    "            print('File downloaded in ' + str(diff.seconds) + 's')\n",
    "            break\n",
    "        else: counter += 1\n",
    "\n",
    "        if counter == len(files):\n",
    "            raise FileNotFoundError('No matching file found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteNetcdf(path):\n",
    "    '''\n",
    "    Deletes the NetCDF-file with the given path\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the file\n",
    "    '''\n",
    "    if path[len(path)-3:len(path)] == \".nc\":\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(\"File deleted: \" + path)\n",
    "        else:\n",
    "            raise FileNotFoundError('No matching file found')\n",
    "    else:\n",
    "        raise NotNetCDFError('Path does not belong to a netCDF-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sst_datacube (yearBegin, yearEnd, directory, name):\n",
    "    '''\n",
    "    The main function to download the sst-data, merge it and safe the datacube\n",
    "\n",
    "    Parameters:\n",
    "        yearBegin (int): First year to download\n",
    "        yearEnd (int): Last year to download\n",
    "        directory (str): Path to the directory\n",
    "        name (str): Name of new File\n",
    "    '''\n",
    "    \n",
    "    '''check parameters'''\n",
    "    if yearBegin > yearEnd or yearBegin == yearEnd:\n",
    "        raise TimeframeError(\"Ending year must be bigger than beginning year\")\n",
    "    '''should catch most invalid filenames'''\n",
    "    invalidCharacters = ['<', '>', ':',  '/',  '|', '*', '?', '\"', '\\\\']\n",
    "    invalidNames = ['CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9']\n",
    "    if len(name) < 1: raise FilenameError(\"Name is not a permitted filename\")\n",
    "    for x in invalidCharacters:\n",
    "        if x in name: raise FilenameError(\"Name is not a permitted filename\")\n",
    "    for x in invalidNames:\n",
    "        if x.lower() == name.lower(): raise FilenameError(\"Name is not a permitted filename\")\n",
    "    if (os.path.exists(directory) == False): raise DirectoryNotFoundError('No matching directory found')\n",
    "    '''download sst data for the wanted years'''\n",
    "    i = yearBegin\n",
    "    files = []\n",
    "    ds_merge = []\n",
    "    while i <= yearEnd:\n",
    "        download_file(i, directory)\n",
    "        files.append(os.path.join(directory,\"sst.day.mean.\" + str(i) + \".nc\"))\n",
    "        i = i + 1\n",
    "    '''merge sst data'''\n",
    "    for f in files:\n",
    "        x = xr.open_dataset(os.path.join(directory, f))\n",
    "        ds_merge.append(x)\n",
    "#     datacube = xr.open_mfdataset(files) '''non dask'''\n",
    "    datacube = xr.open_mfdataset(files, parallel = True, chunks = {\"time\": \"auto\"}) '''with dask'''\n",
    "    '''save datacube'''\n",
    "    print(\"Start saving\")\n",
    "    datacube.to_netcdf(directory + name + \".nc\", compute = True)\n",
    "    print(\"Done saving\")\n",
    "    datacube.close()\n",
    "    '''delete yearly datasets'''\n",
    "    for f in ds_merge:\n",
    "        f.close()\n",
    "    for f in files:\n",
    "        deleteNetcdf(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_sub_datacube (data, timeframe):\n",
    "    '''\n",
    "    Generates a subset along the time dimension of the sst datacube and returns it\n",
    "\n",
    "    Parameters:\n",
    "        data (xArray.Dataset): Dataset to be sliced\n",
    "        timeframe ([str]): Tuple with values for start and end dates, e.g. ['1981-10-01','1981-11-01']\n",
    "        \n",
    "    Returns:\n",
    "        ds (bytes): sub dataset\n",
    "    '''\n",
    "    \n",
    "    if len(timeframe) != 2:\n",
    "        raise TimeframeLengthError(\"Parameter timeframe is an array with two values: [start date, end date]. Please specify an array with exactly two values.\")\n",
    "\n",
    "    try:\n",
    "        x = isinstance(np.datetime64(timeframe[0]),np.datetime64)\n",
    "        x = isinstance(np.datetime64(timeframe[1]),np.datetime64)\n",
    "\n",
    "    except ValueError:\n",
    "        raise ParameterTypeError(\"Values of parameter timeframe must be strings of the format 'year-month-day'. For example '1981-01-01'. Please specify values that follow this.\")\n",
    "\n",
    "    if (type(timeframe[0]) != str or type(timeframe[1]) != str\n",
    "            or len(timeframe[0]) != 10 or len(timeframe[1]) != 10):\n",
    "        raise ParameterTypeError(\"Values of parameter timeframe must be strings of the format 'year-month-day'. For example '1981-01-01'. Please specify values that follow this.\")\n",
    "    elif (timeframe[0] > timeframe[1]\n",
    "            or np.datetime_as_string(data[\"time\"][0], unit='D') > timeframe[0]\n",
    "            or np.datetime_as_string(data[\"time\"][0], unit='D') > timeframe[1]\n",
    "            or timeframe[1] > np.datetime_as_string(data[\"time\"][-1], unit='D')\n",
    "            or timeframe[0] > np.datetime_as_string(data[\"time\"][-1], unit='D')):\n",
    "        raise TimeframeValueError(\"Timeframe values are out of bounds. Please check the range of the dataset.\")\n",
    "    \n",
    "    data_sub = data.sel(time = slice(timeframe[0], timeframe[1]))\n",
    "    data.close()\n",
    "    return data_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load(collection,start, end):\n",
    "    if collection == \"SST\":\n",
    "        cube = xr.open_dataset(os.path.join(directorySST, nameSST))\n",
    "        cube = timeframe(cube, start, end) \n",
    "        return cube\n",
    "\n",
    "    if collection == \"Sentinel2\":\n",
    "        cube = xr.open_dataset(os.path.join(directorySentinel, nameSentinel))\n",
    "        cube = timeframe(cube, start, end) \n",
    "        return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(collection, params):\n",
    "    '''\n",
    "    Executes the SST - or the Sentinel - Dataprocess\n",
    "    \n",
    "    Parameters:\n",
    "        collection (str): The collection which is needed, SST or Sentinel2\n",
    "        params ([]): The params for executing the main - method\n",
    "   '''\n",
    "        \n",
    "    if collection == \"SST\":\n",
    "        yearBegin = params[0]\n",
    "        yearEnd = params[1]\n",
    "        global directorySST \n",
    "        directorySST = params[2]\n",
    "        global name\n",
    "        name = params[3]\n",
    "        generate_sst_datacube(yearBegin, yearEnd, directorySST, name)\n",
    "\n",
    "        \n",
    "    \n",
    "    elif collection == \"Sentinel2\":\n",
    "        resolution = 100\n",
    "        global directorySentinel\n",
    "        directorySentinel = params[0]\n",
    "        collectionDate = params[1]\n",
    "        clouds = params[2]\n",
    "        username = params[3]\n",
    "        password = params[4]\n",
    "        nameSentinel = params[5]\n",
    "        aoi = 'POLYGON((7.52834379254901 52.01238155392252,7.71417925515199 52.01183230436206,7.705255583805303 51.9153349236737,7.521204845259327 51.90983021961716,7.52834379254901 52.01238155392252,7.52834379254901 52.01238155392252))'\n",
    "        mainSentinel(resolution, directorySentinel, collectionDate, aoi, clouds, username, password, nameSentinel)\n",
    "    \n",
    "    else:\n",
    "        raise NameError(\"No Collection named like this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_collection(collection, start, end): \n",
    "    '''\n",
    "    Executes the SST - or the Sentinel - Dataprocess\n",
    "    Parameters:\n",
    "        collection (str): The collection which is needed, SST or Sentinel2\n",
    "        start (datetime64[ns]): startdate of the requested Data\n",
    "        end (datetime64[ns]): enddate of the requested Data\n",
    "    Returns:\n",
    "        datacube (xArray.Dataset): requested datacube\n",
    "    '''\n",
    "\n",
    "    if collection == \"SST\":\n",
    "        if os.path.exists(directorySST+ nameSST+\".nc\"):\n",
    "            SST = xr.open_dataset(directorySST+ nameSST+\".nc\")\n",
    "            timeframe = []\n",
    "            timeframe.append(start)\n",
    "            timeframe.append(end)\n",
    "            SST_sel = get_time_sub_datacube(SST, timeframe)\n",
    "            return SST_sel\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Directory empty\")\n",
    "    elif collection == \"Sentinel2\":\n",
    "        if os.path.exists(directorySentinel + nameSentinel + \".nc\"):\n",
    "            Sentinel = xr.open_dataset(directorySentinel + nameSentinel + \".nc\")\n",
    "            timeframe = []\n",
    "            timeframe.append(start)\n",
    "            timeframe.append(end)\n",
    "            Sentinel_sel = get_time_sub_datacube(Sentinel, timeframe)\n",
    "            return Sentinel_sel\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Directory empty\")\n",
    "    else:\n",
    "        raise NameError(\"No Collection named like this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Params Sentinel'''\n",
    "global directorySentinel \n",
    "directorySentinel = \"C:/Users/adria/Desktop/SentinelData/\"\n",
    "global nameSentinel \n",
    "nameSentinel = \"Sentinel_datacube\"\n",
    "timeframeSentinel = ('2020-06-01T00:00:00Z', '2020-06-15T23:59:59Z')\n",
    "cloud = (0, 30)\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "paramsSentinel = [directorySentinel, timeframeSentinel, cloud, username, password, nameSentinel]\n",
    "\n",
    "'''Params SST'''\n",
    "global directorySST \n",
    "directorySST = \"C:/Users/adria/Desktop/SSTData/\"\n",
    "global nameSST \n",
    "nameSST = 'SST_datacube'\n",
    "SST_start = 2013\n",
    "SST_end = 2018\n",
    "paramsSST = [SST_start, SST_end, directorySST, nameSST]\n",
    "\n",
    "'''Setup'''\n",
    "# create_collection(\"Sentinel2\", paramsSentinel)\n",
    "# create_collection(\"SST\", paramsSST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "User Example\n",
    "'''\n",
    "# Sentinel = load_collection(\"Sentinel2\",'2020-06-01', '2020-06-13')\n",
    "# print(Sentinel)\n",
    "# SST = load_collection(\"SST\",'2013-06-01', '2018-06-03')\n",
    "# print(SST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
